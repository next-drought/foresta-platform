{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Crown Detection using Mask R-CNN\n",
    "\n",
    "This notebook implements a tree crown detection model using Mask R-CNN based on the methodology described in the paper. The model can detect and map tree crowns from Google Earth images.\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tf-nightly (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tf-nightly\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required packages if needed\n",
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install opencv-python==4.7.0.72\n",
    "!pip install scikit-image\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.1.0\n",
      "anyio==4.8.0\n",
      "appnope==0.1.4\n",
      "argon2-cffi==23.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "asttokens==3.0.0\n",
      "astunparse==1.6.3\n",
      "async-lru==2.0.4\n",
      "attrs==25.1.0\n",
      "babel==2.17.0\n",
      "beautifulsoup4==4.13.3\n",
      "bleach==6.2.0\n",
      "certifi==2025.1.31\n",
      "cffi==1.17.1\n",
      "charset-normalizer==3.4.1\n",
      "comm==0.2.2\n",
      "contourpy==1.3.1\n",
      "cycler==0.12.1\n",
      "debugpy==1.8.13\n",
      "decorator==5.2.1\n",
      "defusedxml==0.7.1\n",
      "executing==2.2.0\n",
      "fastjsonschema==2.21.1\n",
      "flatbuffers==25.2.10\n",
      "fonttools==4.56.0\n",
      "fqdn==1.5.1\n",
      "gast==0.6.0\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.70.0\n",
      "h11==0.14.0\n",
      "h5py==3.13.0\n",
      "httpcore==1.0.7\n",
      "httpx==0.28.1\n",
      "idna==3.10\n",
      "imageio==2.37.0\n",
      "ipykernel==6.29.5\n",
      "ipython==9.0.1\n",
      "ipython_pygments_lexers==1.1.1\n",
      "ipywidgets==8.1.5\n",
      "isoduration==20.11.0\n",
      "jedi==0.19.2\n",
      "Jinja2==3.1.5\n",
      "json5==0.10.0\n",
      "jsonpointer==3.0.0\n",
      "jsonschema==4.23.0\n",
      "jsonschema-specifications==2024.10.1\n",
      "jupyter==1.1.1\n",
      "jupyter-console==6.6.3\n",
      "jupyter-events==0.12.0\n",
      "jupyter-lsp==2.2.5\n",
      "jupyter_client==8.6.3\n",
      "jupyter_core==5.7.2\n",
      "jupyter_server==2.15.0\n",
      "jupyter_server_terminals==0.5.3\n",
      "jupyterlab==4.3.5\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.27.3\n",
      "jupyterlab_widgets==3.0.13\n",
      "keras==3.9.0\n",
      "kiwisolver==1.4.8\n",
      "lazy_loader==0.4\n",
      "libclang==18.1.1\n",
      "Markdown==3.7\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==3.0.2\n",
      "-e git+https://github.com/matterport/Mask_RCNN.git@3deaec5d902d16e1daf56b62d5971d428dc920bc#egg=mask_rcnn\n",
      "matplotlib==3.10.1\n",
      "matplotlib-inline==0.1.7\n",
      "mdurl==0.1.2\n",
      "mistune==3.1.2\n",
      "ml-dtypes==0.4.1\n",
      "namex==0.0.8\n",
      "nbclient==0.10.2\n",
      "nbconvert==7.16.6\n",
      "nbformat==5.10.4\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.4.2\n",
      "notebook==7.3.2\n",
      "notebook_shim==0.2.4\n",
      "numpy==2.0.2\n",
      "opencv-python==4.11.0.86\n",
      "opt_einsum==3.4.0\n",
      "optree==0.14.1\n",
      "overrides==7.7.0\n",
      "packaging==24.2\n",
      "pandocfilters==1.5.1\n",
      "parso==0.8.4\n",
      "pexpect==4.9.0\n",
      "pillow==11.1.0\n",
      "platformdirs==4.3.6\n",
      "prometheus_client==0.21.1\n",
      "prompt_toolkit==3.0.50\n",
      "protobuf==5.29.3\n",
      "psutil==7.0.0\n",
      "ptyprocess==0.7.0\n",
      "pure_eval==0.2.3\n",
      "pycparser==2.22\n",
      "Pygments==2.19.1\n",
      "pyparsing==3.2.1\n",
      "python-dateutil==2.9.0.post0\n",
      "python-json-logger==3.2.1\n",
      "PyYAML==6.0.2\n",
      "pyzmq==26.2.1\n",
      "referencing==0.36.2\n",
      "requests==2.32.3\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rich==13.9.4\n",
      "rpds-py==0.23.1\n",
      "scikit-image==0.25.2\n",
      "scipy==1.15.2\n",
      "Send2Trash==1.8.3\n",
      "six==1.17.0\n",
      "sniffio==1.3.1\n",
      "soupsieve==2.6\n",
      "stack-data==0.6.3\n",
      "tensorboard==2.18.0\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorflow==2.18.0\n",
      "tensorflow-io-gcs-filesystem==0.37.1\n",
      "termcolor==2.5.0\n",
      "terminado==0.18.1\n",
      "tifffile==2025.2.18\n",
      "tinycss2==1.4.0\n",
      "tornado==6.4.2\n",
      "traitlets==5.14.3\n",
      "types-python-dateutil==2.9.0.20241206\n",
      "typing_extensions==4.12.2\n",
      "uri-template==1.3.0\n",
      "urllib3==2.3.0\n",
      "wcwidth==0.2.13\n",
      "webcolors==24.11.1\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.8.0\n",
      "Werkzeug==3.1.3\n",
      "widgetsnbextension==4.0.13\n",
      "wrapt==1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip freeze #> requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Mask_RCNN'...\n",
      "remote: Enumerating objects: 956, done.\u001b[K\n",
      "remote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956 (from 1)\u001b[K\n",
      "Receiving objects: 100% (956/956), 137.67 MiB | 9.23 MiB/s, done.\n",
      "Resolving deltas: 100% (558/558), done.\n",
      "Obtaining file:///Users/dynamicpacific/Dropbox/DEV/forestai-platform-model/Mask_RCNN\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: mask-rcnn\n",
      "  Running setup.py develop for mask-rcnn\n",
      "Successfully installed mask-rcnn-2.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Clone Mask RCNN repository if not already installed\n",
    "!git clone https://github.com/matterport/Mask_RCNN.git\n",
    "!pip install -e Mask_RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful!\n"
     ]
    }
   ],
   "source": [
    "# Add Mask_RCNN to Python path\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import skimage.draw\n",
    "import skimage.io\n",
    "import json\n",
    "# Add the repository's root directory to Python path\n",
    "repo_dir = os.path.abspath(\"./Mask_RCNN\")\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.append(repo_dir)\n",
    "\n",
    "# Now try importing\n",
    "from mrcnn.config import Config\n",
    "print(\"Import successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the directory structure and configure paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading COCO weights...\n",
      "Download completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "# Temporarily disable SSL verification (use with caution)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Set file paths\n",
    "ROOT_DIR = os.path.abspath(\"./\")\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "\n",
    "# Download weights if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    print(\"Downloading COCO weights...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\",\n",
    "        COCO_MODEL_PATH\n",
    "    )\n",
    "    print(\"Download completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the existing model to test how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.2.4\n",
      "  Obtaining dependency information for keras==2.2.4 from https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading Keras-2.2.4-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in ./tf_env_py311/lib/python3.11/site-packages (from keras==2.2.4) (2.0.2)\n",
      "Requirement already satisfied: scipy>=0.14 in ./tf_env_py311/lib/python3.11/site-packages (from keras==2.2.4) (1.15.2)\n",
      "Requirement already satisfied: six>=1.9.0 in ./tf_env_py311/lib/python3.11/site-packages (from keras==2.2.4) (1.17.0)\n",
      "Requirement already satisfied: pyyaml in ./tf_env_py311/lib/python3.11/site-packages (from keras==2.2.4) (6.0.2)\n",
      "Requirement already satisfied: h5py in ./tf_env_py311/lib/python3.11/site-packages (from keras==2.2.4) (3.13.0)\n",
      "Collecting keras-applications>=1.0.6 (from keras==2.2.4)\n",
      "  Obtaining dependency information for keras-applications>=1.0.6 from https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl.metadata\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras==2.2.4)\n",
      "  Obtaining dependency information for keras-preprocessing>=1.0.5 from https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.5/312.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras-preprocessing, keras-applications, keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.9.0\n",
      "    Uninstalling keras-3.9.0:\n",
      "      Successfully uninstalled keras-3.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires keras>=3.5.0, but you have keras 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in ./tf_env_py311/lib/python3.11/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: scikit-image in ./tf_env_py311/lib/python3.11/site-packages (0.25.2)\n",
      "Requirement already satisfied: tensorflow in ./tf_env_py311/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./tf_env_py311/lib/python3.11/site-packages (from opencv-python) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.11.4 in ./tf_env_py311/lib/python3.11/site-packages (from scikit-image) (1.15.2)\n",
      "Requirement already satisfied: networkx>=3.0 in ./tf_env_py311/lib/python3.11/site-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: pillow>=10.1 in ./tf_env_py311/lib/python3.11/site-packages (from scikit-image) (11.1.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in ./tf_env_py311/lib/python3.11/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./tf_env_py311/lib/python3.11/site-packages (from scikit-image) (2025.2.18)\n",
      "Requirement already satisfied: packaging>=21 in ./tf_env_py311/lib/python3.11/site-packages (from scikit-image) (24.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in ./tf_env_py311/lib/python3.11/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (2.18.0)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Obtaining dependency information for keras>=3.5.0 from https://files.pythonhosted.org/packages/2b/98/e81c6b2cb522f0eadcc8e16f3cabaccd5462bff6cf52194acfed4a031d3f/keras-3.9.0-py3-none-any.whl.metadata\n",
      "  Using cached keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./tf_env_py311/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./tf_env_py311/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in ./tf_env_py311/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in ./tf_env_py311/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in ./tf_env_py311/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./tf_env_py311/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./tf_env_py311/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./tf_env_py311/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./tf_env_py311/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./tf_env_py311/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./tf_env_py311/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./tf_env_py311/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./tf_env_py311/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./tf_env_py311/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./tf_env_py311/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./tf_env_py311/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Using cached keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.2.4\n",
      "    Uninstalling Keras-2.2.4:\n",
      "      Successfully uninstalled Keras-2.2.4\n",
      "Successfully installed keras-3.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python scikit-image tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning TensorFlow 2.x compatible Mask R-CNN repository...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/Users/dynamicpacific/Dropbox/DEV/forestai-platform-model/TF2-Mask_RCNN'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Mask R-CNN modules\n",
      "Loading weights from /Users/dynamicpacific/Dropbox/DEV/forestai-platform-model/mask_rcnn_coco.h5\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "numpy() is only available when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 186\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# Test on a sample image\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[43mdetect_trees\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/Nursury_Screenshot_GoogleEarth.jpg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/output_detection.png\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mdetect_trees\u001b[39m\u001b[34m(image_path, output_path)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Load pre-trained weights\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading weights from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOCO_MODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOCO_MODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Read and process image\u001b[39;00m\n\u001b[32m    120\u001b[39m image = cv2.imread(image_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/DEV/forestai-platform-model/Mask_RCNN/mrcnn/model.py:2130\u001b[39m, in \u001b[36mMaskRCNN.load_weights\u001b[39m\u001b[34m(self, filepath, by_name, exclude)\u001b[39m\n\u001b[32m   2127\u001b[39m     layers = \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m l: l.name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude, layers)\n\u001b[32m   2129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_name:\n\u001b[32m-> \u001b[39m\u001b[32m2130\u001b[39m     \u001b[43mhdf5_format\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_weights_from_hdf5_group_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2131\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2132\u001b[39m     hdf5_format.load_weights_from_hdf5_group(f, layers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/DEV/forestai-platform-model/tf_env_py311/lib/python3.11/site-packages/tensorflow/python/keras/saving/hdf5_format.py:766\u001b[39m, in \u001b[36mload_weights_from_hdf5_group_by_name\u001b[39m\u001b[34m(f, layers, skip_mismatch)\u001b[39m\n\u001b[32m    763\u001b[39m weight_values = [np.asarray(g[weight_name]) \u001b[38;5;28;01mfor\u001b[39;00m weight_name \u001b[38;5;129;01min\u001b[39;00m weight_names]\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m index.get(name, []):\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m   symbolic_weights = \u001b[43m_legacy_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    767\u001b[39m   weight_values = preprocess_weights_for_loading(\n\u001b[32m    768\u001b[39m       layer, weight_values, original_keras_version, original_backend)\n\u001b[32m    769\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weight_values) != \u001b[38;5;28mlen\u001b[39m(symbolic_weights):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/DEV/forestai-platform-model/tf_env_py311/lib/python3.11/site-packages/tensorflow/python/keras/saving/hdf5_format.py:898\u001b[39m, in \u001b[36m_legacy_weights\u001b[39m\u001b[34m(layer)\u001b[39m\n\u001b[32m    893\u001b[39m weights = layer.trainable_weights + layer.non_trainable_weights\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, variables_module.Variable) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights):\n\u001b[32m    895\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    896\u001b[39m       \u001b[33;43m'\u001b[39;49m\u001b[33;43mSave or restore weights that is not an instance of `tf.Variable` is \u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m    897\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnot supported in h5, use `save_format=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mtf\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m` instead. Got a model \u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mor layer \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[33;43m with weights \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/DEV/forestai-platform-model/tf_env_py311/lib/python3.11/site-packages/keras/src/backend/common/variables.py:372\u001b[39m, in \u001b[36mVariable.__repr__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    370\u001b[39m value = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_value\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     value = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m value_str = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, value=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    375\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<Variable path=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    376\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    377\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/DEV/forestai-platform-model/tf_env_py311/lib/python3.11/site-packages/keras/src/backend/tensorflow/core.py:158\u001b[39m, in \u001b[36mconvert_to_numpy\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf.RaggedTensor):\n\u001b[32m    157\u001b[39m     x = x.to_tensor()\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/DEV/forestai-platform-model/tf_env_py311/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:572\u001b[39m, in \u001b[36mBaseResourceVariable.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Allows direct conversion to a numpy array.\u001b[39;00m\n\u001b[32m    559\u001b[39m \n\u001b[32m    560\u001b[39m \u001b[33;03m>>> np.array(tf.Variable([1.0]))\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    564\u001b[39m \u001b[33;03m  The variable value as a numpy array.\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# You can't return `self.numpy()` here because for scalars\u001b[39;00m\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# that raises:\u001b[39;00m\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m#     ValueError: object __array__ method not producing an array\u001b[39;00m\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# Even `self.read_value().__array__()` and `self.read_value()._numpy()` give\u001b[39;00m\n\u001b[32m    570\u001b[39m \u001b[38;5;66;03m# the same error. The `EagerTensor` class must be doing something behind the\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[38;5;66;03m# scenes to make `np.array(tf.constant(1))` work.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m numpy_compat.np_asarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/DEV/forestai-platform-model/tf_env_py311/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:712\u001b[39m, in \u001b[36mBaseResourceVariable.numpy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly():\n\u001b[32m    711\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_value().numpy()\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    713\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: numpy() is only available when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "# Install TensorFlow Object Detection API if needed\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.system(\"git clone --depth 1 https://github.com/tensorflow/models.git\")\n",
    "    os.system(\"cd models/research && protoc object_detection/protos/*.proto --python_out=.\")\n",
    "    os.system(\"cd models/research && pip install -e .\")\n",
    "\n",
    "# Path to the TensorFlow models directory\n",
    "PATH_TO_MODELS = \"./models\"\n",
    "\n",
    "# Download a pre-trained Mask R-CNN model\n",
    "MODEL_DATE = \"20200711\"\n",
    "MODEL_NAME = \"mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8\"\n",
    "MODEL_PATH = f\"{PATH_TO_MODELS}/research/object_detection/test_data/{MODEL_NAME}/saved_model\"\n",
    "\n",
    "# Download if model doesn't exist\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "    os.system(f\"wget http://download.tensorflow.org/models/object_detection/tf2/20200711/{MODEL_NAME}.tar.gz\")\n",
    "    os.system(f\"tar -xzvf {MODEL_NAME}.tar.gz -C {MODEL_PATH}\")\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model... This might take a minute.\")\n",
    "detect_fn = tf.saved_model.load(MODEL_PATH)\n",
    "\n",
    "# Load label map\n",
    "PATH_TO_LABELS = f\"{PATH_TO_MODELS}/research/object_detection/data/mscoco_label_map.pbtxt\"\n",
    "category_index = label_map_util.create_category_index_from_labelmap(\n",
    "    PATH_TO_LABELS, use_display_name=True)\n",
    "\n",
    "def detect_vegetation(image_path, output_path=None):\n",
    "    \"\"\"Detect vegetation in an image using TensorFlow's Object Detection API.\"\"\"\n",
    "    # Read image\n",
    "    image_np = cv2.imread(image_path)\n",
    "    image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert image to tensor\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.uint8)\n",
    "    \n",
    "    # Run detection\n",
    "    print(\"Running inference...\")\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    # Extract useful data from the result\n",
    "    boxes = detections['detection_boxes'][0].numpy()\n",
    "    classes = detections['detection_classes'][0].numpy().astype(np.int32)\n",
    "    scores = detections['detection_scores'][0].numpy()\n",
    "    masks = None\n",
    "    if 'detection_masks' in detections:\n",
    "        masks = detections['detection_masks'][0].numpy()\n",
    "    \n",
    "    # Define vegetation class IDs (potted plant = 64, tree = 47 in COCO)\n",
    "    vegetation_class_ids = [47, 64]  # COCO IDs for vegetation-related classes\n",
    "    \n",
    "    # Filter for vegetation\n",
    "    vegetation_indices = np.where(np.isin(classes, vegetation_class_ids) & (scores > 0.5))[0]\n",
    "    \n",
    "    # Visualize results\n",
    "    image_np_with_detections = image_np.copy()\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_detections,\n",
    "        boxes[vegetation_indices],\n",
    "        classes[vegetation_indices],\n",
    "        scores[vegetation_indices],\n",
    "        category_index,\n",
    "        instance_masks=masks[vegetation_indices] if masks is not None else None,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=2\n",
    "    )\n",
    "    \n",
    "    # Display and save results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image_np_with_detections)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path, bbox_inches='tight')\n",
    "        print(f\"Detection results saved to {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    num_detections = len(vegetation_indices)\n",
    "    print(f\"Detected {num_detections} vegetation instances\")\n",
    "    \n",
    "    # Calculate mask areas if masks are available\n",
    "    if masks is not None and num_detections > 0:\n",
    "        vegetation_masks = masks[vegetation_indices]\n",
    "        areas = np.sum(vegetation_masks, axis=(1, 2))\n",
    "        avg_area = np.mean(areas)\n",
    "        print(f\"Average vegetation area: {avg_area:.2f} pixels\")\n",
    "    \n",
    "    return {\n",
    "        'num_detections': num_detections,\n",
    "        'classes': classes[vegetation_indices],\n",
    "        'scores': scores[vegetation_indices],\n",
    "        'boxes': boxes[vegetation_indices],\n",
    "        'masks': masks[vegetation_indices] if masks is not None else None\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# detect_vegetation(\"../data/Nursury_Screenshot_GoogleEarth.jpg\", \"../data/output_detection.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz...\n",
      "Extracting model...\n",
      "Loading model...\n",
      "Running inference...\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/dnn/src/net.cpp:105: error: (-215:Assertion failed) !empty() in function 'forward'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 187\u001b[39m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    181\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m: vegetation_count,\n\u001b[32m    182\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mareas\u001b[39m\u001b[33m'\u001b[39m: vegetation_areas,\n\u001b[32m    183\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mprocessed_image\u001b[39m\u001b[33m'\u001b[39m: output_image_rgb\n\u001b[32m    184\u001b[39m     }\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m \u001b[43mdetect_trees\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/Nursury_Screenshot_GoogleEarth.jpg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/output_detection.png\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mdetect_trees\u001b[39m\u001b[34m(image_path, output_path)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Run forward pass\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning inference...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m boxes, masks = \u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdetection_out_final\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdetection_masks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# COCO class IDs for vegetation-related classes:\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# 60: potted plant, 57: carrot, 53: apple, 55: orange, 52: banana\u001b[39;00m\n\u001b[32m     90\u001b[39m vegetation_class_ids = [\u001b[32m60\u001b[39m, \u001b[32m57\u001b[39m, \u001b[32m53\u001b[39m, \u001b[32m55\u001b[39m, \u001b[32m52\u001b[39m]\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/dnn/src/net.cpp:105: error: (-215:Assertion failed) !empty() in function 'forward'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "def download_model():\n",
    "    \"\"\"Download and extract pre-trained Mask R-CNN model for OpenCV.\"\"\"\n",
    "    model_path = \"mask_rcnn_inception_v2_coco_2018_01_28\"\n",
    "    weights_path = f\"{model_path}/frozen_inference_graph.pb\"\n",
    "    \n",
    "    # Check if model already exists\n",
    "    if os.path.exists(weights_path):\n",
    "        print(\"Model files already exist.\")\n",
    "        return weights_path\n",
    "    \n",
    "    # Download model if needed\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        \n",
    "        # Download the model\n",
    "        model_url = \"http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz\"\n",
    "        tar_file = f\"{model_path}.tar.gz\"\n",
    "        \n",
    "        print(f\"Downloading model from {model_url}...\")\n",
    "        urllib.request.urlretrieve(model_url, tar_file)\n",
    "        \n",
    "        # Extract the model\n",
    "        print(\"Extracting model...\")\n",
    "        with tarfile.open(tar_file, \"r:gz\") as tar:\n",
    "            tar.extractall()\n",
    "        \n",
    "        # Clean up\n",
    "        os.remove(tar_file)\n",
    "    \n",
    "    return weights_path\n",
    "\n",
    "def detect_trees(image_path, output_path=None):\n",
    "    \"\"\"Detect vegetation in an image using OpenCV's DNN module with a pre-trained network.\"\"\"\n",
    "    # Download the model\n",
    "    weights_path = download_model()\n",
    "    \n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not load image from {image_path}\")\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # The pre-trained model expects specific input dimensions\n",
    "    # This model expects RGB images in specific format\n",
    "    blob = cv2.dnn.blobFromImage(image, \n",
    "                                swapRB=True, \n",
    "                                crop=False,\n",
    "                                size=(800, 600))\n",
    "    \n",
    "    # Load the network\n",
    "    print(\"Loading model...\")\n",
    "    net = cv2.dnn.readNetFromTensorflow(weights_path)\n",
    "    \n",
    "    # Set the input\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Run forward pass - get all outputs\n",
    "    print(\"Running inference...\")\n",
    "    outs = net.forward()\n",
    "    \n",
    "    # Create class list for COCO dataset\n",
    "    classes = [\"background\", \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \n",
    "               \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \n",
    "               \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \n",
    "               \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \n",
    "               \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \n",
    "               \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\", \n",
    "               \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \n",
    "               \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
    "    \n",
    "    # Vegetation classes\n",
    "    vegetation_class_ids = [\n",
    "        classes.index(\"potted plant\") if \"potted plant\" in classes else -1,\n",
    "        classes.index(\"apple\") if \"apple\" in classes else -1,\n",
    "        classes.index(\"orange\") if \"orange\" in classes else -1,\n",
    "        classes.index(\"broccoli\") if \"broccoli\" in classes else -1,\n",
    "        classes.index(\"carrot\") if \"carrot\" in classes else -1,\n",
    "        classes.index(\"banana\") if \"banana\" in classes else -1\n",
    "    ]\n",
    "    \n",
    "    vegetation_class_ids = [id for id in vegetation_class_ids if id != -1]\n",
    "    \n",
    "    # Define colors for visualization\n",
    "    colors = {\n",
    "        classes.index(\"potted plant\"): (0, 255, 0) if \"potted plant\" in classes else (0, 255, 0),\n",
    "        classes.index(\"apple\"): (0, 100, 255) if \"apple\" in classes else (0, 100, 255),\n",
    "        classes.index(\"orange\"): (0, 165, 255) if \"orange\" in classes else (0, 165, 255),\n",
    "        classes.index(\"broccoli\"): (0, 255, 100) if \"broccoli\" in classes else (0, 255, 100),\n",
    "        classes.index(\"carrot\"): (0, 200, 100) if \"carrot\" in classes else (0, 200, 100),\n",
    "        classes.index(\"banana\"): (0, 255, 255) if \"banana\" in classes else (0, 255, 255)\n",
    "    }\n",
    "    \n",
    "    # Alternative implementation using a simpler detection approach\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    # Use a direct object detection model\n",
    "    from pathlib import Path\n",
    "    weights_dir = Path(\"yolo\")\n",
    "    weights_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Use YOLOv4 which has better compatibility with OpenCV\n",
    "    weights_path = weights_dir / \"yolov4.weights\"\n",
    "    cfg_path = weights_dir / \"yolov4.cfg\"\n",
    "    \n",
    "    if not weights_path.exists() or not cfg_path.exists():\n",
    "        print(\"Downloading YOLOv4 model...\")\n",
    "        urllib.request.urlretrieve(\"https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\", \n",
    "                                  str(weights_path))\n",
    "        urllib.request.urlretrieve(\"https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4.cfg\", \n",
    "                                  str(cfg_path))\n",
    "    \n",
    "    # Load the network\n",
    "    net = cv2.dnn.readNetFromDarknet(str(cfg_path), str(weights_path))\n",
    "    \n",
    "    # Specify the output layers\n",
    "    out_layer_names = net.getUnconnectedOutLayersNames()\n",
    "    \n",
    "    # Prepare the image\n",
    "    blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Run inference\n",
    "    outputs = net.forward(out_layer_names)\n",
    "    \n",
    "    # Get bounding boxes, confidences, and class IDs\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    \n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            # Filter for vegetation classes\n",
    "            if confidence > 0.5 and (class_id in [64, 47, 73, 49]):  # potted plant, tree, apple, orange in YOLO\n",
    "                # Scale the bounding box coordinates to the image size\n",
    "                box = detection[0:4] * np.array([width, height, width, height])\n",
    "                centerX, centerY, box_width, box_height = box.astype(\"int\")\n",
    "                \n",
    "                # Calculate the top-left corner\n",
    "                x = int(centerX - (box_width / 2))\n",
    "                y = int(centerY - (box_height / 2))\n",
    "                \n",
    "                boxes.append([x, y, int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    # Apply non-maximum suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    \n",
    "    # YOLO class names\n",
    "    yolo_classes = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \n",
    "                   \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \n",
    "                   \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \n",
    "                   \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \n",
    "                   \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \n",
    "                   \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \n",
    "                   \"pottedplant\", \"bed\", \"dining table\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \n",
    "                   \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \n",
    "                   \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
    "    \n",
    "    # Define vegetation classes in YOLO\n",
    "    yolo_vegetation = [\"pottedplant\", \"apple\", \"orange\", \"banana\", \"broccoli\", \"carrot\"]\n",
    "    vegetation_indices = [yolo_classes.index(cls) for cls in yolo_vegetation if cls in yolo_classes]\n",
    "    \n",
    "    # Colors for YOLO classes\n",
    "    yolo_colors = {\n",
    "        yolo_classes.index(\"pottedplant\"): (0, 255, 0) if \"pottedplant\" in yolo_classes else (0, 255, 0),\n",
    "        yolo_classes.index(\"apple\"): (0, 100, 255) if \"apple\" in yolo_classes else (0, 100, 255),\n",
    "        yolo_classes.index(\"orange\"): (0, 165, 255) if \"orange\" in yolo_classes else (0, 165, 255),\n",
    "        yolo_classes.index(\"banana\"): (0, 255, 255) if \"banana\" in yolo_classes else (0, 255, 255),\n",
    "        yolo_classes.index(\"broccoli\"): (0, 255, 100) if \"broccoli\" in yolo_classes else (0, 255, 100),\n",
    "        yolo_classes.index(\"carrot\"): (0, 200, 100) if \"carrot\" in yolo_classes else (0, 200, 100)\n",
    "    }\n",
    "    \n",
    "    vegetation_count = 0\n",
    "    vegetation_areas = []\n",
    "    \n",
    "    # Draw the filtered detections\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            # Check if the detection is vegetation\n",
    "            class_id = class_ids[i]\n",
    "            if class_id in vegetation_indices:\n",
    "                vegetation_count += 1\n",
    "                \n",
    "                # Get the box coordinates\n",
    "                x, y, w, h = boxes[i]\n",
    "                \n",
    "                # Calculate area\n",
    "                area = w * h\n",
    "                vegetation_areas.append(area)\n",
    "                \n",
    "                # Draw the bounding box\n",
    "                color = yolo_colors.get(class_id, (0, 255, 0))  # Default to green\n",
    "                cv2.rectangle(output_image, (x, y), (x + w, y + h), color, 2)\n",
    "                \n",
    "                # Add label\n",
    "                label = f\"{yolo_classes[class_id]}: {confidences[i]:.2f}\"\n",
    "                cv2.putText(output_image, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    # Convert to RGB for matplotlib\n",
    "    output_image_rgb = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(output_image_rgb)\n",
    "    plt.title(f\"Tree/Vegetation Detection: {vegetation_count} instances\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path, bbox_inches='tight')\n",
    "        print(f\"Detection results saved to {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Detected {vegetation_count} vegetation instances\")\n",
    "    \n",
    "    if vegetation_areas:\n",
    "        avg_area = np.mean(vegetation_areas)\n",
    "        print(f\"Average vegetation area: {avg_area:.2f} pixels\")\n",
    "    \n",
    "    return {\n",
    "        'count': vegetation_count,\n",
    "        'areas': vegetation_areas,\n",
    "        'processed_image': output_image_rgb\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Define the configuration for the Mask R-CNN model as specified in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeCrownConfig(Config):\n",
    "    \"\"\"Configuration for training on the tree crown dataset.\n",
    "    Derives from the base Config class and overrides specific values.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"tree_crown\"\n",
    "    \n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # Background + Tree Crown\n",
    "    \n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    \n",
    "    # Number of validation steps to run at the end of every training epoch\n",
    "    VALIDATION_STEPS = 50\n",
    "    \n",
    "    # Learning rate and momentum (as described in the paper)\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # Backbone architecture for feature extraction\n",
    "    BACKBONE = \"resnet101\"\n",
    "    \n",
    "    # Input image resizing - keep images with their original aspect ratio\n",
    "    # and enforce a maximum size limit\n",
    "    IMAGE_RESIZE_MODE = \"square\"\n",
    "    IMAGE_MIN_DIM = 800\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    \n",
    "    # ROIs below this threshold are discarded\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Handler\n",
    "\n",
    "The TreeCrownDataset class manages the dataset loading and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeCrownDataset(utils.Dataset):\n",
    "    def load_tree_crowns(self, dataset_dir, subset):\n",
    "        \"\"\"Load a subset of the Tree Crown dataset.\n",
    "        dataset_dir: Root directory of the dataset.\n",
    "        subset: Subset to load: train or val\n",
    "        \"\"\"\n",
    "        # Add classes. We have only one class to add.\n",
    "        self.add_class(\"tree_crown\", 1, \"tree_crown\")\n",
    "        \n",
    "        # Train or validation dataset?\n",
    "        assert subset in [\"train\", \"val\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "        \n",
    "        # Load annotations\n",
    "        # LabelMe format (poly format annotations)\n",
    "        annotations = self.load_labelme_annotations(dataset_dir)\n",
    "        \n",
    "        for a in annotations:\n",
    "            # Get the x, y coordinates of points of the polygons that make up\n",
    "            # the outline of each object instance\n",
    "            polygons = a['polygons']\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            \n",
    "            # Load the image\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "            \n",
    "            self.add_image(\n",
    "                \"tree_crown\",\n",
    "                image_id=a['filename'],  # use file name as a unique image id\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                polygons=polygons)\n",
    "    \n",
    "    def load_labelme_annotations(self, dataset_dir):\n",
    "        \"\"\"Load LabelMe annotations for tree crown polygons.\n",
    "        This is specifically designed for the annotation format\n",
    "        used in the paper with Labelme tool.\n",
    "        \"\"\"\n",
    "        # Implementation would depend on specific format of annotations\n",
    "        # For this example, assuming we have a JSON file for each image\n",
    "        # with polygon coordinates for tree crowns\n",
    "        \n",
    "        annotations = []\n",
    "        \n",
    "        # Scan through all files in the directory\n",
    "        for filename in os.listdir(dataset_dir):\n",
    "            if filename.endswith('.json'):  # Labelme annotations are typically JSON\n",
    "                json_path = os.path.join(dataset_dir, filename)\n",
    "                \n",
    "                # Parse the JSON file\n",
    "                with open(json_path) as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Extract image filename from JSON\n",
    "                image_filename = data['imagePath']\n",
    "                \n",
    "                # Extract polygons - adapt this to match actual Labelme format\n",
    "                polygons = []\n",
    "                for shape in data['shapes']:\n",
    "                    if shape['label'] == 'tree_crown':\n",
    "                        # Convert points to array format\n",
    "                        points = np.array(shape['points'], dtype=np.int32)\n",
    "                        polygons.append(points)\n",
    "                \n",
    "                annotations.append({\n",
    "                    'filename': image_filename,\n",
    "                    'polygons': polygons\n",
    "                })\n",
    "        \n",
    "        return annotations\n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a tree crown dataset image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"tree_crown\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "        \n",
    "        # Convert polygons to a bitmap mask of shape\n",
    "        # [height, width, instance_count]\n",
    "        info = self.image_info[image_id]\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                       dtype=np.uint8)\n",
    "        \n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\n",
    "            rr, cc = skimage.draw.polygon(p[:, 1], p[:, 0])\n",
    "            mask[rr, cc, i] = 1\n",
    "        \n",
    "        # Return mask, and array of class IDs of each instance\n",
    "        return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "    \n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"tree_crown\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Splitting Function\n",
    "\n",
    "This function divides large satellite images into smaller sub-images as described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(image_path, output_dir, tile_size=(935, 910)):\n",
    "    \"\"\"Split a large Google Earth image into smaller sub-images\n",
    "    as described in the paper.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the large image\n",
    "        output_dir: Directory to save the sub-images\n",
    "        tile_size: Size of the sub-images (width, height)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Calculate number of tiles\n",
    "    n_h = math.ceil(h / tile_size[1])\n",
    "    n_w = math.ceil(w / tile_size[0])\n",
    "    \n",
    "    print(f\"Splitting image of size {w}x{h} into {n_w}x{n_h} tiles\")\n",
    "    \n",
    "    # Split the image\n",
    "    count = 0\n",
    "    for i in range(n_h):\n",
    "        for j in range(n_w):\n",
    "            x = j * tile_size[0]\n",
    "            y = i * tile_size[1]\n",
    "            \n",
    "            # Handle edge cases\n",
    "            x_end = min(x + tile_size[0], w)\n",
    "            y_end = min(y + tile_size[1], h)\n",
    "            \n",
    "            # Extract tile\n",
    "            tile = img[y:y_end, x:x_end]\n",
    "            \n",
    "            # Save tile\n",
    "            tile_path = os.path.join(output_dir, f\"tile_{count:03d}.jpg\")\n",
    "            cv2.imwrite(tile_path, tile)\n",
    "            count += 1\n",
    "    \n",
    "    print(f\"Split image into {count} tiles\")\n",
    "    return count\n",
    "\n",
    "# Example usage\n",
    "# split_image(\"large_satellite_image.jpg\", \"tiles/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Function\n",
    "\n",
    "This function implements the training pipeline for the Mask R-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, dataset_dir):\n",
    "    \"\"\"Train the Mask R-CNN model for tree crown detection.\n",
    "    \n",
    "    Args:\n",
    "        config: TreeCrownConfig instance\n",
    "        dataset_dir: Directory containing the dataset\n",
    "    \"\"\"\n",
    "    # Create model in training mode\n",
    "    model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)\n",
    "    \n",
    "    # Load COCO weights as starting point\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True, exclude=[\n",
    "        \"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"\n",
    "    ])\n",
    "    \n",
    "    # Load training dataset\n",
    "    dataset_train = TreeCrownDataset()\n",
    "    dataset_train.load_tree_crowns(dataset_dir, \"train\")\n",
    "    dataset_train.prepare()\n",
    "    \n",
    "    # Load validation dataset\n",
    "    dataset_val = TreeCrownDataset()\n",
    "    dataset_val.load_tree_crowns(dataset_dir, \"val\")\n",
    "    dataset_val.prepare()\n",
    "    \n",
    "    # Train the model\n",
    "    # First, train only the heads (as per the paper's approach)\n",
    "    print(\"Training network heads\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "               learning_rate=config.LEARNING_RATE,\n",
    "               epochs=5,\n",
    "               layers='heads')\n",
    "    \n",
    "    # Fine-tune all layers\n",
    "    print(\"Fine-tuning all layers\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "               learning_rate=config.LEARNING_RATE / 10,\n",
    "               epochs=10,\n",
    "               layers='all')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "# config = TreeCrownConfig()\n",
    "# model = train_model(config, \"dataset_directory/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Function\n",
    "\n",
    "This function detects tree crowns in new images using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_tree_crowns(model, image_path, output_path=None):\n",
    "    \"\"\"Detect tree crowns in an image and save the result.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Mask R-CNN model\n",
    "        image_path: Path to the input image\n",
    "        output_path: Path to save the output visualization\n",
    "        \n",
    "    Returns:\n",
    "        Detection results\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    image = skimage.io.imread(image_path)\n",
    "    \n",
    "    # Detect tree crowns\n",
    "    results = model.detect([image], verbose=1)\n",
    "    r = results[0]\n",
    "    \n",
    "    # Visualize results\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    visualize.display_instances(\n",
    "        image, r['rois'], r['masks'], r['class_ids'],\n",
    "        ['BG', 'Tree Crown'], r['scores'],\n",
    "        title=\"Tree Crown Detection\",\n",
    "        figsize=(12, 12)\n",
    "    )\n",
    "    \n",
    "    # Save the figure if output_path is specified\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    return r\n",
    "\n",
    "# Example usage\n",
    "# detect_tree_crowns(model, \"test_image.jpg\", \"result.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "This function analyzes the detection results to get statistics about tree crowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyze the detection results to get statistics about tree crowns.\n",
    "    \n",
    "    Args:\n",
    "        results: List of detection results for multiple images\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with statistics\n",
    "    \"\"\"\n",
    "    total_trees = 0\n",
    "    total_area = 0\n",
    "    area_distribution = {}\n",
    "    bin_size = 50  # bin size in m²\n",
    "    \n",
    "    for r in results:\n",
    "        n_trees = r['masks'].shape[-1]\n",
    "        total_trees += n_trees\n",
    "        \n",
    "        # Calculate area for each tree crown\n",
    "        for i in range(n_trees):\n",
    "            mask = r['masks'][:, :, i]\n",
    "            area = np.sum(mask) * (0.27**2)  # Convert pixels to m² (0.27m resolution)\n",
    "            total_area += area\n",
    "            \n",
    "            # Update area distribution\n",
    "            bin_idx = int(area / bin_size)\n",
    "            if bin_idx not in area_distribution:\n",
    "                area_distribution[bin_idx] = 0\n",
    "            area_distribution[bin_idx] += 1\n",
    "    \n",
    "    # Prepare distribution data for plotting\n",
    "    area_bins = []\n",
    "    tree_counts = []\n",
    "    for bin_idx in sorted(area_distribution.keys()):\n",
    "        min_area = bin_idx * bin_size\n",
    "        max_area = (bin_idx + 1) * bin_size\n",
    "        area_bins.append(f\"[{min_area}, {max_area})\")\n",
    "        tree_counts.append(area_distribution[bin_idx])\n",
    "    \n",
    "    stats = {\n",
    "        'total_trees': total_trees,\n",
    "        'total_area': total_area,\n",
    "        'area_distribution': {\n",
    "            'bins': area_bins,\n",
    "            'counts': tree_counts\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def plot_area_distribution(stats):\n",
    "    \"\"\"Plot the distribution of tree crown areas.\n",
    "    \n",
    "    Args:\n",
    "        stats: Statistics dictionary returned by analyze_results\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(stats['area_distribution']['bins'], \n",
    "           stats['area_distribution']['counts'])\n",
    "    plt.xlabel('Crown Area (m²)')\n",
    "    plt.ylabel('Number of Trees')\n",
    "    plt.title('Distribution of Tree Crown Areas')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Workflow Example\n",
    "\n",
    "Here's an example of a complete workflow using the functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example workflow - uncomment and modify as needed\n",
    "\n",
    "# 1. Split a large satellite image into smaller tiles\n",
    "# split_image(\"large_satellite_image.jpg\", \"tiles/\")\n",
    "\n",
    "# 2. Create and train the model\n",
    "# config = TreeCrownConfig()\n",
    "# model = train_model(config, \"dataset_directory/\")\n",
    "\n",
    "# 3. Load a trained model for inference\n",
    "# config = TreeCrownConfig()\n",
    "# config.BATCH_SIZE = 1  # For inference\n",
    "# model = modellib.MaskRCNN(mode=\"inference\", config=config, model_dir=MODEL_DIR)\n",
    "# weights_path = model.find_last()  # Find last trained weights\n",
    "# model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 4. Process all images in a directory\n",
    "# results = []\n",
    "# input_dir = \"test_images/\"\n",
    "# output_dir = \"results/\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# for filename in os.listdir(input_dir):\n",
    "#     if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "#         image_path = os.path.join(input_dir, filename)\n",
    "#         output_path = os.path.join(output_dir, f\"result_{os.path.splitext(filename)[0]}.png\")\n",
    "#         print(f\"Processing {image_path}\")\n",
    "#         result = detect_tree_crowns(model, image_path, output_path)\n",
    "#         results.append(result)\n",
    "\n",
    "# 5. Analyze and visualize results\n",
    "# stats = analyze_results(results)\n",
    "# print(f\"Total trees detected: {stats['total_trees']}\")\n",
    "# print(f\"Total crown area: {stats['total_area']:.2f} m²\")\n",
    "# plot_area_distribution(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a Single Image Example\n",
    "\n",
    "You can use this cell to process a single test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for processing a single image\n",
    "# config = TreeCrownConfig()\n",
    "# config.BATCH_SIZE = 1  # For inference\n",
    "# model = modellib.MaskRCNN(mode=\"inference\", config=config, model_dir=MODEL_DIR)\n",
    "# weights_path = model.find_last()  # Find last trained weights\n",
    "# model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# result = detect_tree_crowns(model, \"test_image.jpg\")\n",
    "# print(f\"Detected {result['masks'].shape[-1]} trees\")\n",
    "\n",
    "# # Calculate average crown area\n",
    "# areas = []\n",
    "# for i in range(result['masks'].shape[-1]):\n",
    "#     mask = result['masks'][:, :, i]\n",
    "#     area = np.sum(mask) * (0.27**2)  # Convert pixels to m² (0.27m resolution)\n",
    "#     areas.append(area)\n",
    "\n",
    "# avg_area = np.mean(areas) if areas else 0\n",
    "# print(f\"Average crown area: {avg_area:.2f} m²\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
